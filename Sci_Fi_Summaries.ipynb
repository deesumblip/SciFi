{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3ddbdf",
   "metadata": {},
   "source": [
    "## Summarising multiple Science Fiction titles using NLP transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a69d1",
   "metadata": {},
   "source": [
    "In this project we will take multiple science fiction titles from Project Gutenberg and create shortened summaries using natural language processing. When dealing with large quantities of text, it often helps to have shorter summaries for quick field research. Summarization models also tend to perform better on non-fiction documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70dfd6",
   "metadata": {},
   "source": [
    "First, let's import the books as text data using glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90549e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/dalin/Dropbox/MachineLearning/SciFi/books\\\\A Journey To the Centre of the Earth.txt',\n",
       " 'C:/Users/dalin/Dropbox/MachineLearning/SciFi/books\\\\A Trip to Venus.txt',\n",
       " 'C:/Users/dalin/Dropbox/MachineLearning/SciFi/books\\\\Armageddon.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import library\n",
    "import glob\n",
    "\n",
    "# The books files are contained in this folder\n",
    "folder = \"C:/Users/dalin/Dropbox/MachineLearning/SciFi/books/\"\n",
    "\n",
    "# List all the .txt files and sort them alphabetically\n",
    "files = glob.glob(folder + \"*.txt\")\n",
    "files.sort()\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3adc6",
   "metadata": {},
   "source": [
    "From the Hugging Face library, we will use the default Distilbart-CNN-12-6 model to perform our summarization. Using the Hugging Face pipeline as follows, we'll create a summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8cf08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72083d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84eaeec",
   "metadata": {},
   "source": [
    "Next, we will separate out the texts from the titles of the books. We'll also remove all non-alphanumeric characters, retaining periods, question marks, and exclamation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b76f2f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[492162, 292767, 172609]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import re, os\n",
    "\n",
    "# Initialize the object that will contain the texts and titles\n",
    "txts = []\n",
    "titles = []\n",
    "\n",
    "for n in files:\n",
    "    # Open each file\n",
    "    f = open(n, encoding='utf-8-sig')\n",
    "    # Remove all non-alpha-numeric characters except periods, question marks, exclamation marks. f.read() reads the text and ' ' replaces non-alphanumeric characters with a space.\n",
    "    data = re.sub('[^a-zA-Z0-9_.?!]+', ' ', f.read())\n",
    "    # Store the texts and titles of the books in two separate lists\n",
    "    txts.append(data)\n",
    "    titles.append(os.path.basename(n).replace(\".txt\", \"\"))\n",
    "\n",
    "#Â Print the length, in characters, of each text\n",
    "[len(t) for t in txts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406d9d5",
   "metadata": {},
   "source": [
    "In the next block, we'll create titles for the summaries to identify them once saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acef009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create titles for summary. Will be used at the end for creating text files.\n",
    "summary_string = '_summary.txt'\n",
    "summary_titles = [t + summary_string for t in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e67d87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Journey To the Centre of the Earth_summary.txt',\n",
       " 'A Trip to Venus_summary.txt',\n",
       " 'Armageddon_summary.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61578c",
   "metadata": {},
   "source": [
    "Now we'll extract the main body of each book, excluding introductory and appendix notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d47406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the main body of the text file, excluding introductory and licensing notes, per Project Gutenberg's formatting.\n",
    "content = []\n",
    "for txt in txts:\n",
    "    start = txt.find(\"START OF THIS PROJECT GUTENBERG\") + len(\"START OF THIS PROJECT GUTENBERG\")\n",
    "    end = txt.find(\"END OF THIS PROJECT GUTENBERG\")\n",
    "    substring = txt[start:end]\n",
    "    content.append(substring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104229aa",
   "metadata": {},
   "source": [
    "Let's check the length of each book once extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "893897f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[473512, 274098, 153897]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(c) for c in content]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40331f98",
   "metadata": {},
   "source": [
    "Now for each book, we'll identify the end of sentences using periods, question marks, and exclamation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e2b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_content = []\n",
    "for c in content:\n",
    "    c1 = c.replace('. ', '.<eos>')\n",
    "    c2 = c1.replace('? ', '?<eos>')\n",
    "    c3 = c2.replace('! ', '!<eos>')\n",
    "    eos_content.append(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00171157",
   "metadata": {},
   "source": [
    "Now we'll split the content of the books into individual sentences and feed the sentences in chunks to our summarizer. The summarizer will then generate a summary for book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16a8cfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 120, but you input_length is only 33. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "max_chunk = 500\n",
    "summaries = []\n",
    "for e in eos_content:\n",
    "    sentences = e.split('<eos>')\n",
    "    current_chunk = 0 \n",
    "    chunks = []\n",
    "    for sentence in sentences:\n",
    "        if len(chunks) == current_chunk + 1: \n",
    "            if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "                chunks[current_chunk].extend(sentence.split(' '))\n",
    "            else:\n",
    "                current_chunk += 1\n",
    "                chunks.append(sentence.split(' '))\n",
    "        else:\n",
    "            print(current_chunk)\n",
    "            chunks.append(sentence.split(' '))\n",
    "\n",
    "    for chunk_id in range(len(chunks)):\n",
    "        chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "    \n",
    "    # Summarizer object\n",
    "    res = summarizer(chunks, max_length=120, min_length=30, do_sample=False)\n",
    "    # Join chunk summary together to make whole summary\n",
    "    text = ' '.join([summ['summary_text'] for summ in res])\n",
    "    # Append whole summary to summary list\n",
    "    summaries.append(text)\n",
    "    chunks = []\n",
    "    \n",
    "# Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087c33d",
   "metadata": {},
   "source": [
    "Each book's summary will now be save separately as a text file. Each text file can then be freely opened to grab a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe0d23c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the summaries in individual text files.\n",
    "import io\n",
    "for i in range(len(summaries)):\n",
    "    with io.open(str(titles[i]) + \"_summary.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(str(summaries[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
